{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0cfebb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f8199f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALayer(nn.Module):\n",
    "  \"\"\" \n",
    "    merge:  使用这个线性层的时候,要不要把预训练的权重用上去 | 开关\n",
    "    rank: 降到多少维\n",
    "    lora_alpha: 权重到底以什么样的比例跟原始的权重进行相加\n",
    "  \"\"\"\n",
    "  def __init__(self, in_features, out_features, merge, rank=16, lora_alpha=16, dropout=0.5):\n",
    "    super(LoRALayer, self).__init__()\n",
    "    self.in_features = in_features\n",
    "    self.out_features = out_features\n",
    "    self.rank = rank\n",
    "    self.merge = merge\n",
    "    self.lora_alpha = lora_alpha\n",
    "    self.dropout_rate = dropout\n",
    "\n",
    "    self.linear = nn.Linear(in_features, out_features)\n",
    "\n",
    "    if rank > 0:\n",
    "      self.lora_b = nn.Parameter(torch.zeros(out_features, rank)) # outxr\n",
    "      self.lora_a = nn.Parameter(torch.zeros(rank, in_features)) # rxin\n",
    "      self.scale = self.lora_alpha / self.rank   # 以怎么样一个权重系数,放缩之后的一个结果加到原始的权重当中\n",
    "      self.linear.weight.requires_grad = False  # 冻结原始的权重\n",
    "    \n",
    "    if self.dropout_rate > 0:\n",
    "      self.dropout = nn.Dropout(p=self.dropout_rate)\n",
    "    else:\n",
    "      self.dropout = nn.Identity()\n",
    "    \n",
    "    # 对lora_a权重进行初始化\n",
    "    self.initial_weights()\n",
    "  \n",
    "  def initial_weights(self):\n",
    "    # 把a变成均值为0,方差维sigma的一个高斯噪声分布\n",
    "    nn.init.kaiming_uniform_(self.lora_a, a=math.sqrt(5))\n",
    "    nn.init.zeros_(self.lora_b)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    if self.rank > 0 and self.merge:\n",
    "      # self.linear.weight.shape = (out_features, in_features)\n",
    "      # self.lora_b @ self.lora_a.shape = (out_features, in_features)\n",
    "      output = F.linear(x, self.linear.weight + self.lora_b @ self.lora_a * self.scale, self.linear.bias)\n",
    "      output = self.dropout(output)\n",
    "      return output\n",
    "    else:\n",
    "      return self.dropout(self.linear(x))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l-zh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
