# Low RANK 低秩微调
- LoRA可以直接跟Transformer的FFN层相对齐,不就是说不会对原来的权重造成影响
- 只是要训练一个新的权重加到模型当中就可以了
- 适用于模型本身比较大的情况下,如果模型本身比较小,那就没什么意义了
- 
